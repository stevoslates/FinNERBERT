{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming into NER Dataset\n",
    "Now we have the data into the format we want, we still need to put it into the format of an NER datast. We need it in the format IOB (Inside, Out, Beginning) format for each word. But first we need to create the tags that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "charities_train = pd.read_csv(\"realKieData/charities/train.csv\")\n",
    "charities_validation = pd.read_csv(\"realKieData/charities/val.csv\")\n",
    "charities_test = pd.read_csv(\"realKieData/charities/test.csv\")\n",
    "\n",
    "nda_train = pd.read_csv(\"realKieData/nda/train.csv\")\n",
    "nda_validation = pd.read_csv(\"realKieData/nda/val.csv\")\n",
    "nda_test = pd.read_csv(\"realKieData/nda/test.csv\")\n",
    "\n",
    "fcc_train = pd.read_csv(\"realKieData/fcc/train.csv\")\n",
    "fcc_validation = pd.read_csv(\"realKieData/fcc/val.csv\")\n",
    "fcc_test = pd.read_csv(\"realKieData/fcc/test.csv\")\n",
    "\n",
    "rc_train = pd.read_csv(\"realKieData/rc/train.csv\")\n",
    "rc_validation = pd.read_csv(\"realKieData/rc/val.csv\")\n",
    "rc_test = pd.read_csv(\"realKieData/rc/test.csv\")\n",
    "\n",
    "s1_train = pd.read_csv(\"realKieData/s1/train.csv\")\n",
    "s1_validation = pd.read_csv(\"realKieData/s1/val.csv\")\n",
    "s1_test = pd.read_csv(\"realKieData/s1/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "charities_train['labels'] = charities_train['labels'].apply(json.loads)\n",
    "charities_validation['labels'] = charities_validation['labels'].apply(json.loads)\n",
    "charities_test['labels'] = charities_test['labels'].apply(json.loads)\n",
    "\n",
    "nda_train['labels'] = nda_train['labels'].apply(json.loads)\n",
    "nda_validation['labels'] = nda_validation['labels'].apply(json.loads)\n",
    "nda_test['labels'] = nda_test['labels'].apply(json.loads)\n",
    "\n",
    "fcc_train['labels'] = fcc_train['labels'].apply(json.loads)\n",
    "fcc_validation['labels'] = fcc_validation['labels'].apply(json.loads)\n",
    "fcc_test['labels'] = fcc_test['labels'].apply(json.loads)\n",
    "\n",
    "rc_train['labels'] = rc_train['labels'].apply(json.loads)\n",
    "rc_validation['labels'] = rc_validation['labels'].apply(json.loads)\n",
    "rc_test['labels'] = rc_test['labels'].apply(json.loads)\n",
    "\n",
    "s1_train['labels'] = s1_train['labels'].apply(json.loads)\n",
    "s1_validation['labels'] = s1_validation['labels'].apply(json.loads)\n",
    "s1_test['labels'] = s1_test['labels'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Charity Name',\n",
       " 'start': 2950,\n",
       " 'end': 2970,\n",
       " 'text': 'MICKLEHAM ALMSHOUSES'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charities_train.iloc[0]['labels'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MICKLEHAM ALMSHOUSES'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charities_train.text[0][2950:2970]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating different entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_labels(dataset, column_name='labels'):\n",
    "    all_labels = []\n",
    "    \n",
    "    # Iterate over each row and parse the JSON-like strings\n",
    "    for row in dataset[column_name]:\n",
    "        try:\n",
    "            # Parse the JSON-like string\n",
    "            # Extract labels and add to the all_labels list\n",
    "            for item in row:\n",
    "                all_labels.append(item['label'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON in row: {row} - {e}\")\n",
    "    \n",
    "    # Count the occurrences of each label\n",
    "    label_counts = Counter(all_labels)\n",
    "    \n",
    "    # Print the counts\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charity Name: 4250\n",
      "Charity Registered Number: 729\n",
      "Year Ended: 3644\n",
      "Accounting Basis: 231\n",
      "Independent Examiner Name: 398\n",
      "Examination Date: 211\n",
      "Independent Examiner Street Address: 534\n",
      "Independent Examiner City: 549\n",
      "Independent Examiner Postal Code: 515\n",
      "Net Income at Current Year End: 162\n",
      "Net Income at Previous Year End: 165\n",
      "Net Assets at Current Year End: 242\n",
      "Net Assets at Previous Year End: 231\n",
      "Cash In Hand at Current Year End: 281\n",
      "Cash In Hand at Previous Year End: 273\n",
      "Principal Office Street Address: 294\n",
      "Principal Office City: 295\n",
      "Principal Office Postal Code: 283\n",
      "Trustee Name: 3510\n",
      "Trustee Title: 1051\n",
      "Objectives and Activities: 270\n",
      "Independent Examiner Company: 447\n",
      "Bank Name: 233\n",
      "Named Donor: 445\n",
      "Company Number: 181\n",
      "Named Employee: 52\n",
      "Event Name: 136\n",
      "Project Name: 107\n"
     ]
    }
   ],
   "source": [
    "count_labels(charities_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jurisdiction: 243\n",
      "Party: 522\n",
      "Effective Date: 237\n"
     ]
    }
   ],
   "source": [
    "count_labels(nda_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line Item - Description: 10170\n",
      "Line Item - Days: 8810\n",
      "Line Item - Rate: 11980\n",
      "Line Item - Start Date: 11889\n",
      "Line Item - End Date: 6010\n",
      "Agency: 399\n",
      "Advertiser: 608\n",
      "Gross Total: 496\n",
      "Net Amount Due: 375\n",
      "Agency Commission: 224\n",
      "Payment Terms: 254\n"
     ]
    }
   ],
   "source": [
    "count_labels(fcc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Header) Contract Area Description: 144\n",
      "Contract Area Description: 574\n",
      "(Header)Governing law: 140\n",
      "Governing law: 143\n",
      "(Header)Hardship clause or force majeure: 118\n",
      "Hardship clause or force majeure: 158\n",
      "(Header)Reporting requirements: 301\n",
      "Reporting requirements: 762\n",
      "(Header)Environmental protections: 124\n",
      "Environmental protections: 296\n",
      "(Header)Income tax: rate: 101\n",
      "Income tax: rate: 101\n",
      "(Header)Term: 165\n",
      "Term: 217\n",
      "Renewal or extension of term: 271\n",
      "Type: 202\n",
      "Date Signed: 171\n",
      "Participants: 518\n",
      "Country: 243\n",
      "Project: 117\n",
      "Water use: 72\n",
      "Signatories, company: 283\n",
      "(Header)Water use: 19\n"
     ]
    }
   ],
   "source": [
    "count_labels(rc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Officer: 1528\n",
      "Company Officer Title: 1547\n",
      "Risk Clauses: 14487\n",
      "(Header) Risks To The Business: 194\n",
      "(Header) Description of Securities: 220\n",
      "Description of Securities (1st Para): 226\n",
      "(Header) Dividend Policy: 180\n",
      "Dividend Policy (1st Para): 181\n",
      "(Header) Prospectus Summary: 191\n",
      "Prospectus Summary (1st Para): 1852\n",
      "Joint Book Runners: 352\n",
      "Title of Security Registered: 537\n",
      "Amount Registered: 524\n",
      "Max Price: 280\n",
      "Date of Prospectus: 187\n",
      "Company Name: 195\n",
      "Company Address: 192\n",
      "Agent Name: 190\n",
      "Agent Address: 190\n",
      "Agent Telephone: 183\n",
      "EIN: 187\n",
      "Attorney Names: 738\n",
      "Law Firm Name: 382\n",
      "Law Firm Address: 527\n"
     ]
    }
   ],
   "source": [
    "count_labels(s1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text, removing blank lines and adjusting offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_labels(df, dataset_name, split):\n",
    "    # Filter out rows where labels are equal to '[]' and print them\n",
    "    empty_labels_rows = df[df['labels'] == '[]']\n",
    "    if not empty_labels_rows.empty:\n",
    "        print(f\"Removed rows from {dataset_name} {split}:\")\n",
    "        #print(empty_labels_rows)\n",
    "    \n",
    "    return df[df['labels'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "charities_train = remove_empty_labels(charities_train, 'charities', 'train')\n",
    "charities_validation = remove_empty_labels(charities_validation, 'charities', 'validation')\n",
    "charities_test = remove_empty_labels(charities_test, 'charities', 'test')\n",
    "\n",
    "nda_train = remove_empty_labels(nda_train, 'nda', 'train')\n",
    "nda_validation = remove_empty_labels(nda_validation, 'nda', 'validation')\n",
    "nda_test = remove_empty_labels(nda_test, 'nda', 'test')\n",
    "\n",
    "fcc_train = remove_empty_labels(fcc_train, 'fcc', 'train')\n",
    "fcc_validation = remove_empty_labels(fcc_validation, 'fcc', 'validation')\n",
    "fcc_test = remove_empty_labels(fcc_test, 'fcc', 'test')\n",
    "\n",
    "rc_train = remove_empty_labels(rc_train, 'rc', 'train')\n",
    "rc_validation = remove_empty_labels(rc_validation, 'rc', 'validation')\n",
    "rc_test = remove_empty_labels(rc_test, 'rc', 'test')\n",
    "\n",
    "s1_train = remove_empty_labels(s1_train, 's1', 'train')\n",
    "s1_validation = remove_empty_labels(s1_validation, 's1', 'validation')\n",
    "s1_test = remove_empty_labels(s1_test, 's1', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns apart from text and labels\n",
    "charities_train = charities_train[['text', 'labels']]\n",
    "charities_validation = charities_validation[['text', 'labels']]\n",
    "charities_test = charities_test[['text', 'labels']]\n",
    "\n",
    "nda_train = nda_train[['text', 'labels']]\n",
    "nda_validation = nda_validation[['text', 'labels']]\n",
    "nda_test = nda_test[['text', 'labels']]\n",
    "\n",
    "fcc_train = fcc_train[['text', 'labels']]\n",
    "fcc_validation = fcc_validation[['text', 'labels']]\n",
    "fcc_test = fcc_test[['text', 'labels']]\n",
    "\n",
    "rc_train = rc_train[['text', 'labels']]\n",
    "rc_validation = rc_validation[['text', 'labels']]\n",
    "rc_test = rc_test[['text', 'labels']]\n",
    "\n",
    "s1_train = s1_train[['text', 'labels']]\n",
    "s1_validation = s1_validation[['text', 'labels']]\n",
    "s1_test = s1_test[['text', 'labels']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the entities \n",
    "We only use some of the labels to create 4 entities for the model initally. PER (Person), ORG (Orginisation), LOC (Location), FIN (Finance - monetary amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to make the label to entity mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_entity = {\n",
    "    \"Charity Name\": \"ORG\",\n",
    "    \"Bank Name\": \"ORG\",\n",
    "    \"Party\": \"ORG\",\n",
    "    \"Agency\": \"ORG\",\n",
    "    \"Participants\": \"ORG\",\n",
    "    \"Law Firm Name\": \"ORG\",\n",
    "    \"Company Name\": \"ORG\",\n",
    "    \"Joint Book Runners\": \"ORG\",\n",
    "    \"Company Officer\": \"PER\",\n",
    "    \"Attorney names\" : \"PER\",\n",
    "    \"Independent Examiner Name\": \"PER\",\n",
    "    \"Trustee Name\": \"PER\",\n",
    "    \"Signatories\": \"PER\",\n",
    "    \"Company\": \"PER\",\n",
    "    \"Date Signed\": \"DAT\",\n",
    "    \"Effective Date\": \"DAT\",\n",
    "    \"Examination Date\": \"DAT\",\n",
    "    \"Year Ended\": \"DAT\",\n",
    "    \"Date of Prospectus\": \"DAT\",\n",
    "    \"Line Item - Start Date\": \"DAT\",\n",
    "    \"Line Item - End Date\": \"DAT\",\n",
    "    \"Independent Examiner City\": \"LOC\",\n",
    "    \"Principal Office City\": \"LOC\",\n",
    "    \"Jurisdiction\": \"LOC\",\n",
    "    \"Country\": \"LOC\",\n",
    "    \"Governing Law\": \"LOC\",\n",
    "    \"Net Income at Current Year End\": \"FIN\",\n",
    "    \"Net Income at Previous Year End\": \"FIN\",\n",
    "    \"Net Assets at Current Year End\": \"FIN\",\n",
    "    \"Net Assets at Previous Year End\": \"FIN\",\n",
    "    \"Cash In Hand at Current Year End\": \"FIN\",\n",
    "    \"Cash In Hand at Previous Year End\": \"FIN\",\n",
    "    \"Max Price\": \"FIN\",\n",
    "    \"Net Amount Due\": \"FIN\",\n",
    "    \"Gross Total\": \"FIN\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "#Using BERT fast tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def label_tokens(labels, tokenized_inputs, token_labels):\n",
    "    for label in labels:\n",
    "        start_char = label['start']\n",
    "        end_char = label['end']\n",
    "        label_text = label['label']\n",
    "        \n",
    "        if label_text in label_to_entity:\n",
    "            entity = label_to_entity[label_text]\n",
    "            \n",
    "            for idx, (start, end) in enumerate(tokenized_inputs['offset_mapping']):\n",
    "                if start >= start_char and end <= end_char:\n",
    "                    # Assign B-<entity> for the beginning token and I-<entity> for inside tokens\n",
    "                    if start == start_char:\n",
    "                        token_labels[idx] = f\"B-{entity}\"\n",
    "                    else:\n",
    "                        token_labels[idx] = f\"I-{entity}\"\n",
    "                    \n",
    "    return token_labels\n",
    "\n",
    "def process_dataframe(dataframe):\n",
    "    results = []\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        passage_id = index  # Using the index as the passage ID\n",
    "        text = row['text']\n",
    "        labels = row['labels']\n",
    "        \n",
    "        tokenized_inputs = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=512)\n",
    "        \n",
    "        token_labels = [\"O\"] * len(tokenized_inputs['input_ids'])\n",
    "        \n",
    "        token_labels = label_tokens(labels, tokenized_inputs, token_labels)\n",
    "        \n",
    "        for token, label in zip(tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids']), token_labels):\n",
    "            results.append({\n",
    "                'Passage_ID': passage_id,\n",
    "                'Token': token,\n",
    "                'Label': label\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "charities_test_labeled = process_dataframe(charities_test)\n",
    "charities_train_labeled = process_dataframe(charities_train)\n",
    "charities_validation_labeled = process_dataframe(charities_validation)\n",
    "\n",
    "nda_test_labeled = process_dataframe(nda_test)\n",
    "nda_train_labeled = process_dataframe(nda_train)\n",
    "nda_validation_labeled = process_dataframe(nda_validation)\n",
    "\n",
    "fcc_test_labeled = process_dataframe(fcc_test)\n",
    "fcc_train_labeled = process_dataframe(fcc_train)\n",
    "fcc_validation_labeled = process_dataframe(fcc_validation)\n",
    "\n",
    "rc_test_labeled = process_dataframe(rc_test)\n",
    "rc_train_labeled = process_dataframe(rc_train)\n",
    "rc_validation_labeled = process_dataframe(rc_validation)\n",
    "\n",
    "s1_test_labeled = process_dataframe(s1_test)\n",
    "s1_train_labeled = process_dataframe(s1_train)\n",
    "s1_validation_labeled = process_dataframe(s1_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "charities_train_labeled = process_dataframe(charities_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Passage_ID</th>\n",
       "      <th>Token</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>mick</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>##le</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>##ham</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>al</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163385</th>\n",
       "      <td>321</td>\n",
       "      <td>ms</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163386</th>\n",
       "      <td>321</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163387</th>\n",
       "      <td>321</td>\n",
       "      <td>diane</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163388</th>\n",
       "      <td>321</td>\n",
       "      <td>ru</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163389</th>\n",
       "      <td>321</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163390 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Passage_ID  Token  Label\n",
       "0                0  [CLS]  B-ORG\n",
       "1                0   mick  B-ORG\n",
       "2                0   ##le  I-ORG\n",
       "3                0  ##ham  I-ORG\n",
       "4                0     al  I-ORG\n",
       "...            ...    ...    ...\n",
       "163385         321     ms      O\n",
       "163386         321      .      O\n",
       "163387         321  diane      O\n",
       "163388         321     ru      O\n",
       "163389         321  [SEP]      O\n",
       "\n",
       "[163390 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charities_train_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        93942\n",
       "I-ORG     3035\n",
       "B-ORG      576\n",
       "I-DAT      565\n",
       "B-DAT      184\n",
       "B-PER        1\n",
       "I-PER        1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_train_labeled.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step: Build Datasets\n",
    "Here we build our Ner numerical labels, we also construct the dataset for trainign BERT. Most of the code below for building things like input_ids, attention masks and paddings are taken care of by the HF transformers library, but it was good for my own learning to do this myself and understand BERT deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = {\n",
    "    'O': 0,\n",
    "    'B-PER': 1,\n",
    "    'I-PER': 2,\n",
    "    'B-ORG': 3,\n",
    "    'I-ORG': 4,\n",
    "    'B-LOC': 5,\n",
    "    'I-LOC': 6,\n",
    "    'B-DAT': 7,\n",
    "    'I-DAT': 8,\n",
    "    'B-FIN': 9,\n",
    "    'I-FIN': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Initialize the tokenize\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def group_tokens_to_sentences(df, max_length=512):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    original_sentences = []\n",
    "    ner_tags = []\n",
    "\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "    current_original_sentence = []\n",
    "    current_ner_tags = []\n",
    "    current_length = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        token = row['Token']\n",
    "        label = row['Label']\n",
    "        \n",
    "        tokenized = tokenizer.tokenize(token)\n",
    "        token_length = len(tokenized)\n",
    "        \n",
    "        if current_length + token_length + 2 > max_length:\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(current_sentence)\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            token_type_id = [0] * len(input_ids)\n",
    "            \n",
    "            # Padding\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "            token_type_id += [0] * padding_length\n",
    "            current_labels += [-100] * padding_length  # -100 for padding tokens\n",
    "            \n",
    "            sentences.append(input_ids)\n",
    "            labels.append(current_labels)\n",
    "            attention_masks.append(attention_mask)\n",
    "            token_type_ids.append(token_type_id)\n",
    "            original_sentences.append(current_original_sentence)\n",
    "            ner_tags.append(current_ner_tags)\n",
    "            \n",
    "            current_sentence = []\n",
    "            current_labels = []\n",
    "            current_original_sentence = []\n",
    "            current_ner_tags = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_sentence.extend(tokenized)\n",
    "        current_labels.extend([label_to_index[label]] * token_length)\n",
    "        current_original_sentence.append(token)\n",
    "        current_ner_tags.append(label)\n",
    "        current_length += token_length\n",
    "    \n",
    "    if current_sentence:\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(current_sentence)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        token_type_id = [0] * len(input_ids)\n",
    "        \n",
    "        # Padding\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask += [0] * padding_length\n",
    "        token_type_id += [0] * padding_length\n",
    "        current_labels += [-100] * padding_length  # -100 for padding tokens\n",
    "        \n",
    "        sentences.append(input_ids)\n",
    "        labels.append(current_labels)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        original_sentences.append(current_original_sentence)\n",
    "        ner_tags.append(current_ner_tags)\n",
    "    \n",
    "    sentence_df = pd.DataFrame({\n",
    "        'input_ids': sentences,\n",
    "        'labels': labels,\n",
    "        'attention_masks': attention_masks,\n",
    "        'token_ids': token_type_ids,\n",
    "        'sentence': original_sentences,\n",
    "        'ner_tags': ner_tags\n",
    "    })\n",
    "    \n",
    "    return sentence_df\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "charities_test_gold = group_tokens_to_sentences(charities_test_labeled)\n",
    "charities_train_gold = group_tokens_to_sentences(charities_train_labeled)\n",
    "charities_validation_gold = group_tokens_to_sentences(charities_validation_labeled)\n",
    "\n",
    "nda_test_gold = group_tokens_to_sentences(nda_test_labeled)\n",
    "nda_train_gold = group_tokens_to_sentences(nda_train_labeled)\n",
    "nda_validation_gold = group_tokens_to_sentences(nda_validation_labeled)\n",
    "\n",
    "fcc_test_gold = group_tokens_to_sentences(fcc_test_labeled)\n",
    "fcc_train_gold = group_tokens_to_sentences(fcc_train_labeled)\n",
    "fcc_validation_gold = group_tokens_to_sentences(fcc_validation_labeled)\n",
    "\n",
    "rc_test_gold = group_tokens_to_sentences(rc_test_labeled)\n",
    "rc_train_gold = group_tokens_to_sentences(rc_train_labeled)\n",
    "rc_validation_gold = group_tokens_to_sentences(rc_validation_labeled)\n",
    "\n",
    "s1_test_gold = group_tokens_to_sentences(s1_test_labeled)\n",
    "s1_train_gold = group_tokens_to_sentences(s1_train_labeled)\n",
    "s1_validation_gold = group_tokens_to_sentences(s1_validation_labeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(df, filename):\n",
    "    df.to_json(filename, orient='records', lines=True)\n",
    "\n",
    "save_to_json(charities_test_gold, 'charities_test_gold.json')\n",
    "save_to_json(charities_train_gold, 'charities_train_gold.json')\n",
    "save_to_json(charities_validation_gold, 'charities_validation_gold.json')\n",
    "\n",
    "save_to_json(nda_test_gold, 'nda_test_gold.json')\n",
    "save_to_json(nda_train_gold, 'nda_train_gold.json')\n",
    "save_to_json(nda_validation_gold, 'nda_validation_gold.json')\n",
    "\n",
    "save_to_json(fcc_test_gold, 'fcc_test_gold.json')\n",
    "save_to_json(fcc_train_gold, 'fcc_train_gold.json')\n",
    "save_to_json(fcc_validation_gold, 'fcc_validation_gold.json')\n",
    "\n",
    "save_to_json(rc_test_gold, 'rc_test_gold.json')\n",
    "save_to_json(rc_train_gold, 'rc_train_gold.json')\n",
    "save_to_json(rc_validation_gold, 'rc_validation_gold.json')\n",
    "\n",
    "save_to_json(s1_test_gold, 's1_test_gold.json')\n",
    "save_to_json(s1_train_gold, 's1_train_gold.json')\n",
    "save_to_json(s1_validation_gold, 's1_validation_gold.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- input_ids passed to BERT for trainning\n",
    "- attention_masks also passed to help BERT understand what tokens to pay attention too\n",
    "- labels (the ner labels also passed to BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>ner_labels</th>\n",
       "      <th>attention_masks</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>sentence</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[101, 5952, 3222, 2193, 1011, 2432, 1001, 1001...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[CLS], charity, commission, number, -, 2004, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[10615, 1998, 2968, 1996, 5952, 2001, 2511, 20...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[governance, and, management, the, charity, wa...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5952, 2076, 1996, 2558, 2020, 2004, 4076, 102...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[charity, during, the, period, were, as, follo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-PER, I-PER, I-PER, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1001, 1001, 2002, 2050, 1001, 1001, 16215, 23...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[##hea, ##th, village, hall, for, use, by, loc...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13226, 2502, 1001, 1001, 28177, 1006, 2013, 3...</td>\n",
       "      <td>[1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[vanessa, big, ##gs, (, from, 2nd, april, 2019...</td>\n",
       "      <td>[B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[2004, 2256, 3472, 3855, 1999, 2197, 2095, 100...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[as, our, chairman, mentioned, in, last, year,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[20964, 1001, 1001, 1055, 2410, 2000, 2403, 48...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[auditor, ##s, 13, to, 14, statement, of, fina...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[9360, 11496, 7291, 15730, 5170, 23848, 1001, ...</td>\n",
       "      <td>[0, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[trustees, stephanie, owen, joanna, philip, ma...</td>\n",
       "      <td>[O, B-PER, I-PER, B-PER, I-PER, B-PER, I-PER, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[2043, 2057, 2056, 13407, 2000, 2256, 2336, 32...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[when, we, said, farewell, to, our, children, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[1001, 1001, 2203, 1010, 16889, 1001, 1001, 13...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[##end, ,, hail, ##ing, ,, high, ##am, ,, ho, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_ids  \\\n",
       "0    [101, 5952, 3222, 2193, 1011, 2432, 1001, 1001...   \n",
       "1    [10615, 1998, 2968, 1996, 5952, 2001, 2511, 20...   \n",
       "2    [5952, 2076, 1996, 2558, 2020, 2004, 4076, 102...   \n",
       "3    [1001, 1001, 2002, 2050, 1001, 1001, 16215, 23...   \n",
       "4    [13226, 2502, 1001, 1001, 28177, 1006, 2013, 3...   \n",
       "..                                                 ...   \n",
       "124  [2004, 2256, 3472, 3855, 1999, 2197, 2095, 100...   \n",
       "125  [20964, 1001, 1001, 1055, 2410, 2000, 2403, 48...   \n",
       "126  [9360, 11496, 7291, 15730, 5170, 23848, 1001, ...   \n",
       "127  [2043, 2057, 2056, 13407, 2000, 2256, 2336, 32...   \n",
       "128  [1001, 1001, 2203, 1010, 16889, 1001, 1001, 13...   \n",
       "\n",
       "                                            ner_labels  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, ...   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, ...   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "..                                                 ...   \n",
       "124  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "125  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "126  [0, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...   \n",
       "127  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "128  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       attention_masks  \\\n",
       "0    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "..                                                 ...   \n",
       "124  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "125  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "126  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "127  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "128  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                             token_ids  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "..                                                 ...   \n",
       "124  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "125  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "126  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "127  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "128  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              sentence  \\\n",
       "0    [[CLS], charity, commission, number, -, 2004, ...   \n",
       "1    [governance, and, management, the, charity, wa...   \n",
       "2    [charity, during, the, period, were, as, follo...   \n",
       "3    [##hea, ##th, village, hall, for, use, by, loc...   \n",
       "4    [vanessa, big, ##gs, (, from, 2nd, april, 2019...   \n",
       "..                                                 ...   \n",
       "124  [as, our, chairman, mentioned, in, last, year,...   \n",
       "125  [auditor, ##s, 13, to, 14, statement, of, fina...   \n",
       "126  [trustees, stephanie, owen, joanna, philip, ma...   \n",
       "127  [when, we, said, farewell, to, our, children, ...   \n",
       "128  [##end, ,, hail, ##ing, ,, high, ##am, ,, ho, ...   \n",
       "\n",
       "                                              ner_tags  \n",
       "0    [O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-O...  \n",
       "1    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2    [O, O, O, O, O, O, O, O, B-PER, I-PER, I-PER, ...  \n",
       "3    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4    [B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, ...  \n",
       "..                                                 ...  \n",
       "124  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "125  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "126  [O, B-PER, I-PER, B-PER, I-PER, B-PER, I-PER, ...  \n",
       "127  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "128  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[129 rows x 6 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charities_test_labeled_lists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
